{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e00b4f",
   "metadata": {},
   "source": [
    "# Agent 2 — Module Walkthrough (Code + Review)\n",
    "## Evaluation & Reporting Script (`eval_agent2.py`)\n",
    "\n",
    "**Author:** Summer Xiong  \n",
    "**Purpose:**  \n",
    "Evaluate **Agent 2** on the validation split and export **paper‑ready artifacts**, including:\n",
    "- overall and per‑cluster metrics\n",
    "- confusion matrices (raw + normalised)\n",
    "- calibration (ECE + reliability curves)\n",
    "- LaTeX tables for direct inclusion in the dissertation / paper\n",
    "\n",
    "This script is *post‑training* and assumes a trained Agent 2 model stored in `agent2_artifacts/`.\n",
    "\n",
    "> **Key idea:**  \n",
    "> Training performance alone is insufficient. This module focuses on **interpretability, calibration, and fairness across clusters**, which directly supports your dissertation claims.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5be743",
   "metadata": {},
   "source": [
    "## 1) Command‑Line Interface (CLI)\n",
    "\n",
    "```bash\n",
    "python eval_agent2.py \\\n",
    "  --data_dir path/to/data \\\n",
    "  --artifacts_dir path/to/agent2_artifacts\n",
    "```\n",
    "\n",
    "### Arguments\n",
    "- `--code_dir`: where Agent2 modules live (dataset.py, model.py, etc.)\n",
    "- `--data_dir`: directory containing `cluster_0/1/2_dataset.csv`\n",
    "- `--artifacts_dir`: trained model + config (auto‑discovered if empty)\n",
    "- `--output_dir`: where evaluation outputs are saved\n",
    "- `--seed`: **must match training seed** (critical)\n",
    "- `--batch_size`: evaluation batch size\n",
    "\n",
    "⚠️ **Methodological note**  \n",
    "The same `seed` and `split_by_voter` logic as training are reused → avoids leakage and guarantees consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c033a",
   "metadata": {},
   "source": [
    "## 2) Artifact Auto‑Discovery\n",
    "\n",
    "### Problem addressed\n",
    "In practice (especially Colab), model artifacts may not live in a fixed path.\n",
    "\n",
    "### Solution\n",
    "`autodiscover_artifacts()`:\n",
    "- checks common relative locations\n",
    "- falls back to a *light recursive search* under `/content` and Drive\n",
    "- verifies both `config.json` and `agent2_model.pt` exist\n",
    "\n",
    "This makes the evaluation script robust and user‑friendly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62dd9e",
   "metadata": {},
   "source": [
    "## 3) Data Reloading & Canonical Normalisation\n",
    "\n",
    "### Steps\n",
    "1. Reload raw CSVs\n",
    "2. Apply `normalise_columns`\n",
    "3. Filter valid labels `{0,1,2}`\n",
    "\n",
    "### Why reload instead of reusing training tensors?\n",
    "- guarantees **stateless reproducibility**\n",
    "- allows independent verification of results\n",
    "- aligns with best practices for empirical research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0336f1d",
   "metadata": {},
   "source": [
    "## 4) Model & Tokenizer Reconstruction (Critical Section)\n",
    "\n",
    "### Key steps\n",
    "1. Load `config.json`\n",
    "2. Assert **label mapping consistency**\n",
    "3. Load tokenizer (with special tokens)\n",
    "4. Instantiate model with correct `feat_dim`\n",
    "5. **Resize token embeddings BEFORE loading weights**\n",
    "6. Load model state dict\n",
    "\n",
    "```python\n",
    "model.text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(state)\n",
    "```\n",
    "\n",
    "⚠️ **This ordering is critical**  \n",
    "Failing to resize embeddings first will cause a shape mismatch error.\n",
    "\n",
    "This section demonstrates strong engineering discipline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a92af",
   "metadata": {},
   "source": [
    "## 5) Rebuilding Validation Windows\n",
    "\n",
    "### Why rebuild windows?\n",
    "Agent 2 windows are:\n",
    "- voter‑dependent\n",
    "- time‑ordered\n",
    "- seed‑dependent\n",
    "\n",
    "To ensure comparability with training:\n",
    "- `split_by_voter` is re‑run with the same seed\n",
    "- windows are rebuilt using identical logic\n",
    "\n",
    "This ensures **evaluation integrity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ded204",
   "metadata": {},
   "source": [
    "## 6) Inference Loop\n",
    "\n",
    "### Outputs collected\n",
    "- `y_true`: true labels\n",
    "- `y_pred`: predicted labels\n",
    "- `probs_all`: softmax probabilities\n",
    "- `clusters`: cluster id per window\n",
    "\n",
    "These are the foundation for:\n",
    "- performance metrics\n",
    "- confusion matrices\n",
    "- calibration analysis\n",
    "- cluster‑level fairness evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ecf2b",
   "metadata": {},
   "source": [
    "## 7) Overall Performance Reports\n",
    "\n",
    "### Generated artifacts\n",
    "- `classification_report.csv`\n",
    "- `confusion_matrix.csv`\n",
    "- `confusion_matrix_normalised.csv`\n",
    "- `confusion_matrix_blue.png`\n",
    "\n",
    "### Design choice\n",
    "- **Blue color theme** → consistent, paper‑friendly\n",
    "- Normalised confusion matrix → easier class‑imbalance interpretation\n",
    "\n",
    "These outputs are immediately usable in the Results section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324eb9e",
   "metadata": {},
   "source": [
    "## 8) Calibration: ECE & Reliability Curves\n",
    "\n",
    "### Why calibration matters\n",
    "A model can be accurate but **over‑confident or under‑confident**.\n",
    "In governance settings, confidence miscalibration can distort decision‑making.\n",
    "\n",
    "### Implemented\n",
    "- Expected Calibration Error (ECE)\n",
    "- Reliability curves per class:\n",
    "  - For\n",
    "  - Against\n",
    "  - Abstain\n",
    "- Probability histograms\n",
    "\n",
    "### Outputs\n",
    "- `prob_calibration_ece.csv`\n",
    "- `reliability_*_blue.png`\n",
    "- `prob_hist_*_blue.png`\n",
    "\n",
    "This directly strengthens the *trustworthiness* argument of your agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0e379",
   "metadata": {},
   "source": [
    "## 9) Per‑Cluster Evaluation (Fairness Analysis)\n",
    "\n",
    "### Motivation\n",
    "Clusters represent **behavioural voter types** (from Agent 1).\n",
    "\n",
    "Evaluating only overall metrics can hide:\n",
    "- systematic under‑performance on minority clusters\n",
    "- governance fairness issues\n",
    "\n",
    "### What is computed per cluster\n",
    "- Macro‑Precision\n",
    "- Macro‑Recall\n",
    "- Macro‑F1\n",
    "- Accuracy\n",
    "- Confusion matrices (raw + normalised)\n",
    "\n",
    "### Outputs\n",
    "- `confusion_matrix_cluster_{k}.csv`\n",
    "- `confusion_matrix_cluster_{k}_normalised.csv`\n",
    "- `confusion_matrix_cluster_{k}_blue.png`\n",
    "\n",
    "This is a **core contribution** of your dissertation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7464abd",
   "metadata": {},
   "source": [
    "## 10) Macro‑Level Summary Table (Paper‑Ready)\n",
    "\n",
    "### Generated files\n",
    "- `macro_by_cluster.csv`\n",
    "- `latex_macro_by_cluster.tex`\n",
    "\n",
    "The LaTeX table uses:\n",
    "- `booktabs`\n",
    "- fixed precision\n",
    "- consistent ordering\n",
    "\n",
    "This table can be dropped **directly into Overleaf** with no edits.\n",
    "\n",
    "It summarises:\n",
    "- Overall performance\n",
    "- Per‑cluster performance\n",
    "→ making cross‑group comparison explicit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa443d3",
   "metadata": {},
   "source": [
    "## 11) Review Notes (Strengths & Improvements)\n",
    "\n",
    "### ✅ Strengths\n",
    "- Full separation of training vs evaluation\n",
    "- Strong reproducibility guarantees\n",
    "- Rich evaluation beyond accuracy\n",
    "- Cluster‑aware fairness analysis\n",
    "- Publication‑ready outputs (CSV + PNG + LaTeX)\n",
    "\n",
    "### ⚠️ Possible Extensions\n",
    "1. Add **bootstrap confidence intervals** for macro‑F1\n",
    "2. Evaluate **temperature‑scaled logits** explicitly\n",
    "3. Add **per‑cluster ECE**\n",
    "4. Log runtime & GPU memory usage\n",
    "5. Automate figure numbering to match dissertation sections\n",
    "\n",
    "These are optional but would further strengthen a journal submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e97c7",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "This evaluation module transforms Agent 2 from:\n",
    "> *“a trained model”*  \n",
    "into  \n",
    "> **“a scientifically evaluated, interpretable, and fair governance agent.”**\n",
    "\n",
    "It provides:\n",
    "- rigorous validation\n",
    "- fairness diagnostics\n",
    "- calibration analysis\n",
    "- paper‑ready artifacts\n",
    "\n",
    "This is exactly the level expected for a **master’s dissertation and beyond**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
