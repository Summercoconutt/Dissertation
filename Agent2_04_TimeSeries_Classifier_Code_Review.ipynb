{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77352e78",
   "metadata": {},
   "source": [
    "# Agent 2 — Module Walkthrough (Code + Review)\n",
    "## Time-Series Classifier (`model.py`)\n",
    "\n",
    "**Author:** Summer Xiong  \n",
    "**Goal:** Explain the architecture and forward pass of the Agent 2 model, including tensor shapes and design rationale.\n",
    "\n",
    "This module defines `TimeSeriesClassifier`, a hybrid model that combines:\n",
    "1. **Pretrained text encoder** (RoBERTa via `AutoModel`) applied per step  \n",
    "2. **Numeric feature projection** applied per step  \n",
    "3. **Temporal Transformer encoder** over the window dimension `W`  \n",
    "4. A **classification head** that predicts the current-step vote label (For/Against/Abstain)\n",
    "\n",
    "> **Key idea:** Each training sample is a window of `W` steps. Each step has:\n",
    "> - proposal text (tokenised)\n",
    "> - numeric features (vp, vp_share, time features, etc.)\n",
    "> The model encodes each step and then models temporal dependencies across steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6b9b0",
   "metadata": {},
   "source": [
    "## 0) Imports\n",
    "\n",
    "- `torch`, `torch.nn`: model components and training\n",
    "- `transformers.AutoModel`: load a pretrained transformer encoder (e.g., RoBERTa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54212821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343f41e",
   "metadata": {},
   "source": [
    "## 1) Class Overview: `TimeSeriesClassifier`\n",
    "\n",
    "### Constructor parameters (what they control)\n",
    "- `pretrained_model_name`: text encoder backbone (default: `roberta-base`)\n",
    "- `hidden_dim`: projected text representation size per step\n",
    "- `feat_dim`: numeric feature dimension per step (must match your window builder output)\n",
    "- `label_emb_dim`: embedding dim for label tokens (see review note below)\n",
    "- `pos_emb_dim`: embedding dim for step positions in the window\n",
    "- `num_heads`, `ff_dim`: Transformer encoder hyperparameters\n",
    "- `num_classes`: 3 by default (For/Against/Abstain)\n",
    "- `dropout`: regularisation\n",
    "\n",
    "### High-level computation graph\n",
    "For each batch:\n",
    "1. Encode each step text with RoBERTa → take `[CLS]` token representation  \n",
    "2. Project numeric features through an MLP  \n",
    "3. Add position embedding (and optionally label embedding)  \n",
    "4. Concatenate all step features → run temporal Transformer over steps  \n",
    "5. Take last step representation → classify into 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_model_name: str = \"roberta-base\",\n",
    "                 hidden_dim: int = 256,\n",
    "                 feat_dim: int = 8,\n",
    "                 label_emb_dim: int = 128,\n",
    "                 pos_emb_dim: int = 128,\n",
    "                 num_heads: int = 8,\n",
    "                 ff_dim: int = 1024,\n",
    "                 num_classes: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, hidden_dim)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # label/position embeddings\n",
    "        self.label_emb = nn.Embedding(num_classes + 1, label_emb_dim)  # +1 for current-step placeholder\n",
    "        self.pos_emb = nn.Embedding(512, pos_emb_dim)                  # support up to W<=512\n",
    "\n",
    "        # numeric projection\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "\n",
    "        fused_dim = hidden_dim + label_emb_dim + pos_emb_dim + 128\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=fused_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.temporal = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # temperature (optional calibration)\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d66314",
   "metadata": {},
   "source": [
    "## 2) Component-by-Component Explanation\n",
    "\n",
    "### 2.1 Text encoder (`self.text_encoder`)\n",
    "A pretrained transformer (RoBERTa by default) outputs contextual token embeddings.\n",
    "\n",
    "You extract the representation of the first token `[:, 0, :]` (often treated as a sentence embedding):\n",
    "- shape per step: `(H_text,)`\n",
    "- `H_text = self.text_encoder.config.hidden_size`\n",
    "\n",
    "Then project it to `hidden_dim` using `self.text_proj` to:\n",
    "- reduce dimensionality\n",
    "- control the fused representation size\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Numeric feature projection (`self.feat_proj`)\n",
    "`feat_proj` maps per-step numeric features of size `feat_dim` into a 128-dimensional embedding.\n",
    "\n",
    "This makes numeric features comparable in scale/expressiveness to text embeddings.\n",
    "\n",
    "**Important coupling:** `feat_dim` must exactly match the number of per-step numeric features produced by your window builder.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Position embedding (`self.pos_emb`)\n",
    "Encodes step index `0..W-1` into `pos_emb_dim`.\n",
    "\n",
    "This gives the temporal transformer awareness of step order.\n",
    "\n",
    "Note: you hardcode support for `W<=512`. If your window size is always small (e.g., 5–50), this is fine.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Label embedding (`self.label_emb`)\n",
    "This is defined, but in `forward()` you currently set all label tokens to zero.  \n",
    "So *as implemented*, it is effectively a constant vector added to every step.\n",
    "\n",
    "This is not harmful, but it is not providing meaningful information unless you supply step-level label ids.\n",
    "\n",
    "See review notes in Section 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7efeac",
   "metadata": {},
   "source": [
    "## 3) Forward Pass (Step-by-step with Shapes)\n",
    "\n",
    "The `batch` dictionary is expected to contain:\n",
    "- `input_ids`: `(B, W, L)`\n",
    "- `attention_mask`: `(B, W, L)`\n",
    "- `num_feats`: `(B, W, F)` where `F = feat_dim`\n",
    "- `labels`: `(B,)`\n",
    "- `clusters`: `(B,)`\n",
    "\n",
    "The model returns:\n",
    "- `logits`: `(B, C)` where `C=num_classes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5996edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    batch: input_ids (B,W,L), attention_mask (B,W,L), num_feats (B,W,F), labels (B,), clusters (B,)\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]      # (B,W,L)\n",
    "    attn = batch[\"attention_mask\"]      # (B,W,L)\n",
    "    feats = batch[\"num_feats\"]          # (B,W,F)\n",
    "    B, W, L = input_ids.size()\n",
    "\n",
    "    # flatten steps for text encoding\n",
    "    x = input_ids.view(B * W, L)\n",
    "    a = attn.view(B * W, L)\n",
    "    enc = self.text_encoder(input_ids=x, attention_mask=a)\n",
    "\n",
    "    cls = enc.last_hidden_state[:, 0, :]              # (B*W, Htext)\n",
    "    cls = self.text_proj(cls)                         # (B*W, hidden_dim)\n",
    "    cls = cls.view(B, W, -1)                          # (B, W, hidden_dim)\n",
    "\n",
    "    # label & position embeddings\n",
    "    pos_ids = torch.arange(W, device=input_ids.device).unsqueeze(0).expand(B, W)\n",
    "    pos = self.pos_emb(pos_ids)                       # (B, W, pos_emb_dim)\n",
    "\n",
    "    # label embeddings (currently all zeros)\n",
    "    label_tokens = torch.zeros((B, W), dtype=torch.long, device=input_ids.device)\n",
    "    lab = self.label_emb(label_tokens)                # (B, W, label_emb_dim)\n",
    "\n",
    "    # numeric projection\n",
    "    fproj = self.feat_proj(feats)                     # (B, W, 128)\n",
    "\n",
    "    # fuse\n",
    "    fused = torch.cat([cls, lab, pos, fproj], dim=-1) # (B, W, fused_dim)\n",
    "\n",
    "    # temporal modelling over steps\n",
    "    seq = self.temporal(fused)                        # (B, W, fused_dim)\n",
    "\n",
    "    # classify using the last step representation (current step)\n",
    "    last = seq[:, -1, :]                              # (B, fused_dim)\n",
    "    logits = self.cls_head(last)                      # (B, num_classes)\n",
    "\n",
    "    return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9885c",
   "metadata": {},
   "source": [
    "### 3.1 Why flatten `B*W` for text encoding?\n",
    "\n",
    "The pretrained text encoder expects shape `(batch, seq_len)`.  \n",
    "Your data has an extra window dimension `(B, W, L)`, so you reshape to:\n",
    "- `(B*W, L)` to encode all steps in one pass\n",
    "\n",
    "Then you reshape back to `(B, W, hidden_dim)`.\n",
    "\n",
    "This is efficient and correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b90e1b",
   "metadata": {},
   "source": [
    "## 4) Loss Function and Calibration\n",
    "\n",
    "### 4.1 `loss_fn`\n",
    "Uses cross-entropy loss, optionally with `class_weights` to address class imbalance.\n",
    "\n",
    "### 4.2 `calibrate_logits`\n",
    "Implements **temperature scaling**: `logits / T`.\n",
    "\n",
    "This is typically used post-training for calibration (ECE improvement) but can also be learned jointly.\n",
    "\n",
    "Important: you are returning raw logits in `forward`; calibration is applied externally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c854673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(self, logits: torch.Tensor, labels: torch.Tensor, class_weights: Optional[torch.Tensor] = None):\n",
    "    if class_weights is not None:\n",
    "        loss = nn.CrossEntropyLoss(weight=class_weights)(logits, labels)\n",
    "    else:\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def calibrate_logits(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "    # temperature scaling\n",
    "    return logits / self.temperature.clamp(min=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87281d3b",
   "metadata": {},
   "source": [
    "## 5) Review Notes (Strengths, Risks, and Improvements)\n",
    "\n",
    "### ✅ Strengths\n",
    "- Clear separation of modalities: text vs numeric\n",
    "- Good use of pretrained encoder for proposal semantics\n",
    "- Temporal transformer models cross-step dependencies\n",
    "- Uses last-step representation to predict current vote (aligned with window design)\n",
    "- Includes an explicit calibration parameter (temperature)\n",
    "\n",
    "### ⚠️ Key risks / potential issues\n",
    "1) **Label embedding is currently unused**\n",
    "   - In `forward()`, `label_tokens` is all zeros for all steps.\n",
    "   - That means `lab` is constant, adding no information.\n",
    "\n",
    "   **If you intended to use history labels**, you have two options:\n",
    "   - Provide per-step label ids as part of the batch (e.g., from window builder), and set:\n",
    "     - history steps: true label ids (0/1/2)\n",
    "     - current step: placeholder id = `num_classes` (the extra +1)\n",
    "   - Or remove label embeddings entirely to simplify.\n",
    "\n",
    "2) **`feat_dim` must match actual numeric vector length**\n",
    "   - Your sliding window builder creates feature vectors:\n",
    "     - `len(numeric_cols)` + `(#boolean cols included)` + `4 time features`\n",
    "   - This must exactly equal `feat_dim`, otherwise training will crash.\n",
    "\n",
    "   **Recommendation:** assert `feats.size(-1) == feat_dim` at runtime.\n",
    "\n",
    "3) **Position embedding max length**\n",
    "   - `self.pos_emb = nn.Embedding(512, ...)` supports `W<=512`.\n",
    "   - If you ever increase `window_size` beyond 512, this will break.\n",
    "\n",
    "4) **CLS pooling assumption**\n",
    "   - You use `last_hidden_state[:, 0, :]` as sentence embedding.\n",
    "   - Works for RoBERTa, but you may also consider mean pooling for robustness.\n",
    "\n",
    "5) **Calibration usage**\n",
    "   - Temperature scaling is usually fitted on validation set after training.\n",
    "   - If you train `temperature` jointly, ensure you are not leaking validation info.\n",
    "\n",
    "---\n",
    "\n",
    "### Suggested short-term improvements (publication-ready)\n",
    "- Implement history label ids properly *or* remove label_emb\n",
    "- Add assertions for shapes:\n",
    "  - `input_ids.shape == (B, W, L)`\n",
    "  - `num_feats.shape[-1] == feat_dim`\n",
    "- Log parameter choices and report ablations:\n",
    "  - with/without label prefixes in text\n",
    "  - with/without explicit label embeddings\n",
    "  - with/without numeric features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1f3b9",
   "metadata": {},
   "source": [
    "## 6) Minimal Shape Sanity Check (No Transformers Download)\n",
    "\n",
    "Because `AutoModel.from_pretrained(...)` downloads weights, this notebook provides a **non-downloading shape test** pattern.\n",
    "\n",
    "In your real training environment, you would instantiate:\n",
    "```python\n",
    "model = TimeSeriesClassifier(pretrained_model_name=\"roberta-base\", feat_dim=F)\n",
    "```\n",
    "\n",
    "Below we define a tiny dummy module to test tensor flow without external downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.config = type(\"cfg\", (), {\"hidden_size\": hidden_size})()\n",
    "        self.emb = nn.Embedding(1000, hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids: (N, L)\n",
    "        x = self.emb(input_ids)  # (N, L, H)\n",
    "        return type(\"out\", (), {\"last_hidden_state\": x})()\n",
    "\n",
    "class TimeSeriesClassifier_NoDownload(TimeSeriesClassifier):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(nn.Module, self).__init__()\n",
    "        # mimic original init without downloading\n",
    "        pretrained_model_name = kwargs.get(\"pretrained_model_name\", \"dummy\")\n",
    "        hidden_dim = kwargs.get(\"hidden_dim\", 16)\n",
    "        feat_dim = kwargs.get(\"feat_dim\", 8)\n",
    "        label_emb_dim = kwargs.get(\"label_emb_dim\", 8)\n",
    "        pos_emb_dim = kwargs.get(\"pos_emb_dim\", 8)\n",
    "        num_heads = kwargs.get(\"num_heads\", 2)\n",
    "        ff_dim = kwargs.get(\"ff_dim\", 64)\n",
    "        num_classes = kwargs.get(\"num_classes\", 3)\n",
    "        dropout = kwargs.get(\"dropout\", 0.1)\n",
    "\n",
    "        self.text_encoder = DummyTextEncoder(hidden_size=32)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, hidden_dim)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.label_emb = nn.Embedding(num_classes + 1, label_emb_dim)\n",
    "        self.pos_emb = nn.Embedding(512, pos_emb_dim)\n",
    "\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "\n",
    "        fused_dim = hidden_dim + label_emb_dim + pos_emb_dim + 128\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=fused_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.temporal = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # paste the original forward logic\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attn = batch[\"attention_mask\"]\n",
    "        feats = batch[\"num_feats\"]\n",
    "        B, W, L = input_ids.size()\n",
    "\n",
    "        x = input_ids.view(B * W, L)\n",
    "        a = attn.view(B * W, L)\n",
    "        enc = self.text_encoder(input_ids=x, attention_mask=a)\n",
    "\n",
    "        cls = enc.last_hidden_state[:, 0, :]\n",
    "        cls = self.text_proj(cls)\n",
    "        cls = cls.view(B, W, -1)\n",
    "\n",
    "        pos_ids = torch.arange(W, device=input_ids.device).unsqueeze(0).expand(B, W)\n",
    "        pos = self.pos_emb(pos_ids)\n",
    "\n",
    "        label_tokens = torch.zeros((B, W), dtype=torch.long, device=input_ids.device)\n",
    "        lab = self.label_emb(label_tokens)\n",
    "\n",
    "        fproj = self.feat_proj(feats)\n",
    "\n",
    "        fused = torch.cat([cls, lab, pos, fproj], dim=-1)\n",
    "        seq = self.temporal(fused)\n",
    "        last = seq[:, -1, :]\n",
    "        logits = self.cls_head(last)\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "# Dummy batch\n",
    "B, W, L, F = 2, 3, 8, 8\n",
    "batch = {\n",
    "    \"input_ids\": torch.randint(0, 1000, (B, W, L)),\n",
    "    \"attention_mask\": torch.ones((B, W, L), dtype=torch.long),\n",
    "    \"num_feats\": torch.randn((B, W, F)),\n",
    "    \"labels\": torch.randint(0, 3, (B,)),\n",
    "    \"clusters\": torch.randint(0, 3, (B,)),\n",
    "}\n",
    "\n",
    "m = TimeSeriesClassifier_NoDownload(feat_dim=F)\n",
    "out = m(batch)\n",
    "out[\"logits\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46762ede",
   "metadata": {},
   "source": [
    "## 7) Summary\n",
    "\n",
    "`TimeSeriesClassifier` is a multimodal temporal model:\n",
    "- **RoBERTa** encodes proposal texts per step\n",
    "- an MLP embeds numeric signals per step\n",
    "- a **TransformerEncoder** models temporal dependencies across the window\n",
    "- a classification head predicts the current vote (3-class)\n",
    "\n",
    "Key implementation checkpoint: ensure `feat_dim` matches your window feature vector length, and decide whether to properly use history labels in `label_emb` or remove it for simplicity.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
