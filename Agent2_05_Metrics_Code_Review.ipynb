{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97ca3df",
   "metadata": {},
   "source": [
    "# Agent 2 — Module Walkthrough (Code + Review)\n",
    "## Metrics (`metrics.py`)\n",
    "\n",
    "**Author:** Summer Xiong  \n",
    "**Goal:** Explain the evaluation metric used in Agent 2, why it is chosen, and how it should be interpreted in the context of DAO vote prediction.\n",
    "\n",
    "This module defines a single function:\n",
    "- `macro_prf`: computes **macro-averaged precision, recall, and F1-score**\n",
    "\n",
    "> **Key idea:** In multi-class vote prediction (For / Against / Abstain), class imbalance is common.  \n",
    "> Macro-averaged metrics treat each class equally, making them suitable for fairness-oriented evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645581e7",
   "metadata": {},
   "source": [
    "## 0) Imports\n",
    "\n",
    "- `numpy`: numerical safety / type casting\n",
    "- `sklearn.metrics.precision_recall_fscore_support`: standard multi-class PRF computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00433b37",
   "metadata": {},
   "source": [
    "## 1) `macro_prf(y_true, y_pred)`\n",
    "\n",
    "```python\n",
    "def macro_prf(y_true, y_pred) -> Dict[str,float]:\n",
    "    p,r,f,_ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\"precision\": float(p), \"recall\": float(r), \"f1\": float(f)}\n",
    "```\n",
    "\n",
    "### What this function does\n",
    "Computes **macro-averaged**:\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "\n",
    "across all classes.\n",
    "\n",
    "### Input / Output\n",
    "- **Input**\n",
    "  - `y_true`: ground-truth labels, shape `(N,)`\n",
    "  - `y_pred`: predicted labels, shape `(N,)`\n",
    "- **Output**\n",
    "  - dictionary with keys: `precision`, `recall`, `f1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_prf(y_true, y_pred) -> Dict[str, float]:\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\"precision\": float(p), \"recall\": float(r), \"f1\": float(f)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e8e42",
   "metadata": {},
   "source": [
    "## 2) Why Macro-Averaged Metrics?\n",
    "\n",
    "### Class imbalance in DAO voting\n",
    "In many DAOs:\n",
    "- **FOR** votes dominate\n",
    "- **AGAINST** and **ABSTAIN** are relatively rare\n",
    "\n",
    "If you used **micro-averaged** or accuracy-based metrics:\n",
    "- a model predicting mostly FOR could look artificially strong\n",
    "- minority-class performance would be hidden\n",
    "\n",
    "### Macro averaging\n",
    "Macro-averaged PRF:\n",
    "- computes precision/recall/F1 **per class**\n",
    "- then averages them equally across classes\n",
    "\n",
    "This aligns well with:\n",
    "- fairness considerations\n",
    "- governance analysis\n",
    "- your dissertation theme (efficiency *and* fairness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c8f10",
   "metadata": {},
   "source": [
    "## 3) `zero_division=0`: What it Means\n",
    "\n",
    "### The problem\n",
    "If a class is never predicted:\n",
    "- precision for that class is undefined (division by zero)\n",
    "\n",
    "### Your choice\n",
    "```python\n",
    "zero_division=0\n",
    "```\n",
    "\n",
    "This means:\n",
    "- undefined precision/recall is set to 0 instead of raising an error\n",
    "\n",
    "### Why this is reasonable\n",
    "- Penalises models that completely ignore a class\n",
    "- Keeps training/evaluation loops robust\n",
    "- Avoids silent metric inflation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074dcd2",
   "metadata": {},
   "source": [
    "## 4) How This Metric Fits into Agent 2\n",
    "\n",
    "### Typical usage pattern\n",
    "During validation or testing:\n",
    "1. Collect logits from the model\n",
    "2. Convert logits → predicted class ids via `argmax`\n",
    "3. Call `macro_prf(y_true, y_pred)`\n",
    "4. Log or store the resulting dictionary\n",
    "\n",
    "This metric should be reported:\n",
    "- per epoch (validation)\n",
    "- for final test evaluation\n",
    "- optionally **per cluster** (using `cluster_id`) for fairness analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8317b",
   "metadata": {},
   "source": [
    "## 5) Minimal Sanity Check Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465579cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sanity check\n",
    "y_true = np.array([0, 0, 1, 1, 2, 2])\n",
    "y_pred = np.array([0, 0, 0, 1, 2, 2])\n",
    "\n",
    "macro_prf(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809895d7",
   "metadata": {},
   "source": [
    "## 6) Review Notes & Possible Extensions\n",
    "\n",
    "### ✅ Strengths\n",
    "- Minimal, clear, and robust\n",
    "- Uses a well-understood sklearn implementation\n",
    "- Macro averaging aligns with imbalance-aware evaluation\n",
    "\n",
    "### ⚠️ Potential Extensions\n",
    "1) **Per-class metrics**\n",
    "   - return precision/recall/F1 per class for deeper diagnostics\n",
    "2) **Confusion matrix**\n",
    "   - useful for qualitative error analysis\n",
    "3) **Calibration metrics**\n",
    "   - Expected Calibration Error (ECE)\n",
    "   - complements your temperature scaling in the model\n",
    "4) **Cluster-conditional metrics**\n",
    "   - compute macro PRF per `cluster_id` to study behavioural fairness\n",
    "\n",
    "These extensions can live in the same `metrics.py` or a separate evaluation module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc92ff7",
   "metadata": {},
   "source": [
    "## 7) Summary\n",
    "\n",
    "This module defines the core evaluation metric for Agent 2.  \n",
    "By using **macro-averaged precision, recall, and F1**, it ensures that:\n",
    "- minority voting behaviours are not ignored\n",
    "- reported performance reflects balanced predictive ability\n",
    "- evaluation aligns with governance fairness objectives\n",
    "\n",
    "It is a simple but methodologically sound choice.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
